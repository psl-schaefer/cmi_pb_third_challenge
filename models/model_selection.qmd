---
title: "Model Selection"
author: "Philipp Sven Lars Sch√§fer"
date: "`r format(Sys.time(), '%d %B, %Y')`"
editor: source
engine: knitr
---

# Packages

```{r}
suppressPackageStartupMessages({
  library(tidyverse)
  library(flextable)
  library(ggdark)
  library(magick)
  library(glmnet)
  library(ranger)
})

source(file.path("..", "src", "read_data.R"))
source(file.path("..", "src", "colors.R"))
source(file.path("..", "src", "generate_targets.R"))
source(file.path("..", "src", "model.R"))
```

# Data

```{r}
input_dir = file.path("..", "data")
output_dir = file.path("..", "results")
dir.create(output_dir, showWarnings = FALSE)
```

```{r}
celltype_meta <- read_celltype_meta(input_dir)
gene_meta <- read_gene_meta_plus(input_dir)
protein_meta <- read_protein_meta(input_dir)

meta_data <- read_harmonized_meta_data(input_dir)
specimen_per_day <- get_specimen_per_day(meta_data=meta_data)

RECOMPUTE <- FALSE
if (RECOMPUTE) {
  source(file.path("..", "src", "normalize_integrate.R"))
  
  raw_experimental_data <- read_raw_experimental_data(input_dir)
  
  filtered_experimental_data <- filter_experimental_data(
    meta_data=meta_data, 
    experimental_data=raw_experimental_data,
    gene_meta=gene_meta)
  
  write_rds(filtered_experimental_data, 
            file = file.path(input_dir, "prc_datasets", 
                             "filtered_experimental_data.RDS"))
  
  normalized_experimental_data <- normalize_experimental_data(
    meta_data=meta_data, 
    raw_experimental_data=filtered_experimental_data,
    gene_meta=gene_meta)
  
  write_rds(normalized_experimental_data, 
            file = file.path(input_dir, "prc_datasets", 
                             "normalized_experimental_data.RDS"))
  
  integrated_experimental_data <- integrate_experimental_data(
    meta_data=meta_data, 
    normalized_experimental_data=normalized_experimental_data)
  
  write_rds(integrated_experimental_data, 
            file = file.path(input_dir, "prc_datasets", 
                             "integrated_experimental_data.RDS"))

  # use raw/filtered experimental data for computation of targets
  target_list <- generate_all_targets(
    meta_data=meta_data, 
    experimental_data=filtered_experimental_data, 
    experimental_data_settings=experimental_data_settings, 
    gene_meta=gene_meta,
    protein_meta=protein_meta
    )
  
  write_rds(target_list, 
          file = file.path(input_dir, "prc_datasets", 
                           "target_list.RDS"))
  
  rm(raw_experimental_data, filtered_experimental_data)
  experimental_data <- integrated_experimental_data
} else {
  experimental_data <- read_rds(file = file.path(input_dir, "prc_datasets", 
                                                            "integrated_experimental_data.RDS"))
  experimental_data <- experimental_data[-which(names(experimental_data) ==
                                                "pbmc_gene_expression_counts")]
  target_list <- read_rds(file = file.path(input_dir, "prc_datasets", 
                                           "target_list.RDS"))
}
```

# Tasks

```{r}
task_meta <- list(
  task_11 = list(
    name = "task_11",
    header = "## Task 1.1",
    description = "Rank the individuals by IgG antibody levels against pertussis toxin (PT) that we detect in plasma 14 days post booster vaccinations."
  ),
  task_12 = list(
    name = "task_12",
    header = "## Task 1.2",
    description = "Rank the individuals by fold change of IgG antibody levels against pertussis toxin (PT) that we detect in plasma 14 days post booster vaccinations compared to titer values at day 0."
  ),
  task_21 = list(
    name = "task_21",
    header = "## Task 2.1",
    description = "Rank the individuals by predicted frequency of Monocytes on day 1 post boost after vaccination."
  ),
  task_22 = list(
    name = "task_22",
    header = "## Task 2.2",
    description = "Rank the individuals by fold change of predicted frequency of Monocytes on day 1 post booster vaccination compared to cell frequency values at day 0."
  ),
  task_31 = list(
    name = "task_31",
    header = "## Task 3.1",
    description = "Rank the individuals by predicted gene expression of CCL3 on day 3 post-booster vaccination."
  ),
  task_32 = list(
    name = "task_32",
    header = "## Task 3.2",
    description = "Rank the individuals by fold change of predicted gene expression of CCL3 on day 3 post booster vaccination compared to gene expression values at day 0."
  ),
  task_41 = list(
    name = "task_41",
    header = "## Task 4.1",
    description = "Rank the individuals based on their Th1/Th2 (IFN-g/IL-5) polarization ratio on Day 30 post-booster vaccination."
  )
)
```

# Conclusions

- 

# Prepare Features

```{r}
N_HVG <- 2000

hvg <- gene_meta %>% 
  dplyr::mutate("prefix_versioned_ensembl_gene_id_clean" =
                  paste0("pbmc_gene_expression_", versioned_ensembl_gene_id_clean)) %>%
  dplyr::slice_max(mean_rank, n=N_HVG, with_ties=TRUE)

meta_data_cov <- get_metadata_covariates(meta_data) %>%
  dplyr::select(-c(ethnicity_Not_Hispanic_or_Latino, ethnicity_Hispanic_or_Latino))

experimental_predictors_day_0 <- 
  generate_wide_experimental_data(experimental_data=experimental_data,
                                  impute="median", 
                                  verbose=TRUE) %>%
  purrr::imap(function(df, modality) {
    colnames(df) <- paste0(modality, "_", colnames(df))
    df <- as.data.frame(df) %>%
      tibble::rownames_to_column("specimen_id") %>%
      dplyr::mutate(specimen_id = as.numeric(specimen_id)) %>%
      dplyr::left_join((specimen_per_day$day_0 %>% 
                        dplyr::select(subject_id, specimen_id, dataset)),
                       by="specimen_id") %>%
      dplyr::select(-c(specimen_id, dataset)) %>%
      dplyr::filter(!is.na(subject_id)) %>%
      dplyr::select(subject_id, everything())
    return(df)
  })
experimental_predictors_day_0$pbmc_gene_expression <-
  experimental_predictors_day_0$pbmc_gene_expression[, c("subject_id", hvg$prefix_versioned_ensembl_gene_id_clean)]

#purrr::map(experimental_predictors_day_0, ~ dim(.x))

experimental_predictors_day_0_pca <- experimental_predictors_day_0 %>%
  purrr::imap(function(df, modality) {
    # df <- experimental_predictors_day_0$
    mtx <- df %>% dplyr::select(-subject_id) %>% as.matrix()
    pca <- stats::prcomp(mtx)
    sdev_ratio <- cumsum(pca$sdev) / sum(pca$sdev)
    n_pcs <- max(which(sdev_ratio < 0.9))
    df_pca <- as.data.frame(pca$x[, 1:n_pcs]) %>%
      dplyr::mutate(subject_id = df$subject_id) %>%
      dplyr::select(subject_id, everything())
    return(df_pca)
})

get_powerset <- function(x) {
  # powerset excluding the empty set!
  sets <- lapply(1:(length(x)), \(i) utils::combn(x, i, simplify = FALSE))
  unlist(sets, recursive = FALSE)
}

modality_powerset <- get_powerset(names(experimental_predictors_day_0))

#purrr::map(experimental_predictors_day_0_pca, ~ dim(.x))

feature_list <- list(
  "metadata" = meta_data_cov,
  "metadata+baseline" = meta_data_cov # will take care of that logic in loop
)

# adding power set of single-omic pca predictors
for (modality_set in modality_powerset) {
  # modality_set <- modality_powerset[[100]]
  modality_set_df <- meta_data_cov
  
  modality_set_name <- paste0(paste0(modality_set, collapse = "_pca+"), "_pca+metadata+baseline")
  feature_list[[modality_set_name]] <- 
    experimental_predictors_day_0_pca[modality_set] %>%
    purrr::reduce(inner_join, by="subject_id") %>% # should we use inner? or outer with impute?
    dplyr::left_join(meta_data_cov, by="subject_id")
}

# adding power set of single-omic predictors
for (modality_set in modality_powerset) {
  # modality_set <- modality_powerset[[100]]
  modality_set_df <- meta_data_cov
  
  modality_set_name <- paste0(paste0(modality_set, collapse = "+"), "+metadata+baseline")
  feature_list[[modality_set_name]] <- 
    experimental_predictors_day_0[modality_set] %>%
    purrr::reduce(inner_join, by="subject_id") %>% # should we use inner? or outer with impute?
    dplyr::left_join(meta_data_cov, by="subject_id")
}

#purrr::map(feature_list, ~ dim(.x))
```

# Loop

```{r}
tictoc::tic()

result_list <-  list()

for (task in task_meta) {
  # task <- task_meta[[1]]
  
  task_df <- target_list[[task$name]]
  
  target_df <- task_df %>%
    dplyr::select(subject_id, target)
  
  baseline_df <- task_df %>% 
    dplyr::select(subject_id, baseline)
  
  baseline_model_df <- task_df %>%
    dplyr::left_join(dplyr::distinct(dplyr::select(meta_data, c(subject_id, dataset))),
                     by="subject_id")
  
  all_datasets <- unique(baseline_model_df$dataset)
  
  for (test_dataset in all_datasets) {
    result_list <- rlist::list.append(result_list, tibble::tibble(
      task = task$name,
      trainset = NA,
      testset = test_dataset,
      test_subset = task_df$subject_id[baseline_model_df$dataset==test_dataset],
      target = task_df$target[baseline_model_df$dataset==test_dataset], 
      prediction = task_df$baseline[baseline_model_df$dataset==test_dataset],
      testset_mean = NA,
      model = "baseline",
      feature_set = "baseline"
    ))
  }
  
  #for (model_name in c("lm", "rf", "lasso", "elnet", "lm+boruta", "rf+boruta")) {
  #for (model_name in c("lasso")) {
  #for (model_name in c("lm")) {
  for (model_name in c("lm", "rf", "lasso", "elnet", "rf+boruta")) {
    # model_name <- "lasso"
    for (feature_name in names(feature_list)) {
      # feature_name <- names(feature_list)[2]
      feature_df <- feature_list[[feature_name]]
      
      if (str_detect(feature_name, "baseline")) {
        feature_df <- feature_df %>%
          dplyr::left_join(baseline_df, by="subject_id")
      }
      
      model_df <- target_df %>%
        dplyr::left_join(feature_df, by="subject_id") %>%
        dplyr::left_join(dplyr::distinct(dplyr::select(meta_data, c(subject_id, dataset))),
                         by="subject_id")
      
      #stopifnot(!any(is.na(model_df))) # guard rail
      model_df <- model_df %>% tidyr::drop_na()
      stopifnot(nrow(model_df)==nrow(dplyr::distinct(model_df)))
      
      all_datasets <- unique(model_df$dataset)
      
      cross_dataset_out <- purrr::map(all_datasets, function(test_dataset) {
        # test_dataset <- all_datasets[2]
        train_dataset <- all_datasets[!all_datasets %in% test_dataset]
        
        train_df <- model_df %>%
          dplyr::filter(dataset %in% train_dataset) %>%
          dplyr::select(-c(dataset, subject_id))
        
        train_mtx <- as.matrix(dplyr::select(train_df, -target))
        train_target <- dplyr::pull(train_df, target)
        
        test_df_with_subject <- model_df %>%
          dplyr::filter(dataset %in% test_dataset)
        
        test_df <- test_df_with_subject %>%
          dplyr::select(-c(dataset, subject_id))
        
        test_mtx <- as.matrix(dplyr::select(test_df, -target))
        test_target <- dplyr::pull(test_df, target)
        
        if (model_name == "rf") {
          model <- ranger::ranger(formula = target ~ ., 
                                  data = train_df, 
                                  num.trees = 500)
          predictions <- predict(model, test_df)$predictions
        } 
        else if (model_name == "rf+boruta") {
          if (ncol(train_df) > 2) {
            boruta_out <- Boruta::Boruta(formula = target ~ .,
                                         data = train_df,
                                         maxRuns=1000,
                                         num.trees=500 
                                         )
            boruta_feats <- names(boruta_out$finalDecision)[boruta_out$finalDecision=="Confirmed"]
          } else {
            boruta_feats <- colnames(train_df)[colnames(train_df)!="target"]
          }
          if (length(boruta_feats) == 0) {
            predictions <- rep(mean(train_df$target), nrow(test_df))
          } else {
            train_df_subset <- train_df %>% dplyr::select(dplyr::all_of(c(boruta_feats, "target")))
            test_df_subset <- test_df %>% dplyr::select(dplyr::all_of(c(boruta_feats, "target")))
            model <- ranger::ranger(formula = target ~ ., 
                                    data = train_df_subset, 
                                    num.trees = 500)
            predictions <- predict(model, test_df_subset)$predictions
          }
        } 
        else if (model_name == "lm") {
          # standardize features if using linear model
          train_df_lm <- train_df
          test_df_lm <- test_df
          for (column in colnames(train_df_lm)[colnames(train_df_lm)!="target"]) {
            column_mean <- mean(train_df_lm[[column]])
            column_sd <- sd(train_df_lm[[column]])
            train_df_lm[[column]] <- (train_df_lm[[column]] - column_mean) / column_sd
            test_df_lm[[column]] <- (test_df_lm[[column]] - column_mean) / column_sd
          }
          design_matrix <- model.matrix(target ~ ., data = train_df_lm)
          rank <- qr(design_matrix)$rank
          num_predictors <- ncol(design_matrix)
          # If rank is less than number of predictors, return constant vector
          if (rank < num_predictors) {
            predictions <- rep(mean(train_df_lm$target), nrow(test_df_lm))
          } else {
            # Otherwise, fit the model and make predictions
            model <- stats::lm(formula = target ~ ., data = train_df_lm, singular.ok = FALSE)
            predictions <- predict(model, test_df_lm)
          }
        } 
        else if (model_name == "lm+boruta") {
          # standardize features if using linear model
          train_df_lm <- train_df
          test_df_lm <- test_df
          for (column in colnames(train_df_lm)[colnames(train_df_lm)!="target"]) {
            column_mean <- mean(train_df_lm[[column]])
            column_sd <- sd(train_df_lm[[column]])
            train_df_lm[[column]] <- (train_df_lm[[column]] - column_mean) / column_sd
            test_df_lm[[column]] <- (test_df_lm[[column]] - column_mean) / column_sd
          }
          if (ncol(train_df) > 2) {
            boruta_out <- Boruta::Boruta(formula = target ~ .,
                                         data = train_df,
                                         maxRuns=1000,
                                         num.trees=500 
                                         )
            boruta_feats <- names(boruta_out$finalDecision)[boruta_out$finalDecision=="Confirmed"]
          } else {
            boruta_feats <- colnames(train_df)[colnames(train_df)!="target"]
          }
          train_df_subset <- train_df_lm %>% dplyr::select(dplyr::all_of(c(boruta_feats, "target")))
          test_df_subset <- test_df_lm %>% dplyr::select(dplyr::all_of(c(boruta_feats, "target")))
          design_matrix <- model.matrix(target ~ ., data = train_df_subset)
          rank <- qr(design_matrix)$rank
          num_predictors <- ncol(design_matrix)
          # If rank is less than number of predictors, return constant vector
          if (rank < num_predictors) {
            predictions <- rep(mean(train_df_subset$target), nrow(test_df_subset))
          } else {
            # Otherwise, fit the model and make predictions
            model <- stats::lm(formula = target ~ ., data = train_df_subset, singular.ok = FALSE)
            predictions <- predict(model, test_df_subset)
          }
        } 
        else if (model_name == "lasso") {
          # standardize the features, even though glmnet is doing this automatically!
          feature_means <- matrixStats::colMeans2(train_mtx)
          feature_sds <- matrixStats::colSds(train_mtx)
          train_mtx <- t((t(train_mtx) - feature_means) / feature_sds)
          train_target <- (train_target - mean(train_target)) / sd(train_target)
          #test_mtx <- t((t(test_mtx) - feature_means) / feature_sds)
          
          # inner hyperparameter optim loop using LOOCV to find best lambda
          model <- glmnet::cv.glmnet(x=train_mtx,
                                     y=train_target,
                                     nfolds=nrow(train_mtx),
                                     grouped=FALSE,
                                     alpha=1, # LASSO
                                     type.measure="mae", # not sure whether this is better
                                     standardize=TRUE, # by default each feature is standardized internally
                                     family="gaussian"
                                     )
          # investigate
          #beta_mtx <- as.data.frame(as.matrix(model$glmnet.fit$beta))
          #colnames(beta_mtx) <- paste0("lambda_", round(model$lambda, 3))
          #plot(model)
          #lambda_value <- model$lambda.1se, # model$lambda.min
          # never use zero features (with lamba=model$lambda[1]), since constant predictions
          # will have NA spearman...
          lambda_value <- ifelse(model$lambda.min==model$lambda[1], 
                                 model$lambda[2], model$lambda.min)
          predictions <- predict(model,
                                 newx=test_mtx, 
                                 s=lambda_value)[,1]
        }
        else if (model_name == "elnet") {
          # standardize the features, even though glmnet is doing this automatically!
          feature_means <- matrixStats::colMeans2(train_mtx)
          feature_sds <- matrixStats::colSds(train_mtx)
          train_mtx <- t((t(train_mtx) - feature_means) / feature_sds)
          train_target <- (train_target - mean(train_target)) / sd(train_target)
          #test_mtx <- t((t(test_mtx) - feature_means) / feature_sds)
          
          # inner hyperparameter optim loop using LOOCV to find best lambda
          model <- glmnet::cv.glmnet(x=train_mtx,
                                     y=train_target,
                                     nfolds=nrow(train_mtx),
                                     grouped=FALSE,
                                     alpha=0.5, # elastic net
                                     type.measure="mae", # not sure whether this is better
                                     standardize=TRUE, # by default each feature is standardized internally
                                     family="gaussian"
                                     )
          # investigate
          #beta_mtx <- as.data.frame(as.matrix(model$glmnet.fit$beta))
          #colnames(beta_mtx) <- paste0("lambda_", round(model$lambda, 3))
          #plot(model)
          #lambda_value <- model$lambda.1se, # model$lambda.min
          # never use zero features (with lamba=model$lambda[1]), since constant predictions
          # will have NA spearman...
          lambda_value <- ifelse(model$lambda.min==model$lambda[1], 
                                 model$lambda[2], model$lambda.min)
          predictions <- predict(model,
                                 newx=test_mtx, 
                                 s=lambda_value)[,1]
        }
        else {
          stop(paste0(model_name), " not implemented")
        }
        
        tibble::tibble(task=task$name,
                       trainset = paste0(train_dataset, collapse="__"),
                       testset = test_dataset,
                       test_subset = test_df_with_subject$subject_id,
                       target = test_df$target, 
                       prediction = predictions,
                       testset_mean = mean(test_df$target),
                       model = model_name,
                       feature_set = feature_name)
      }) %>%
        dplyr::bind_rows()
      result_list <- rlist::list.append(result_list, cross_dataset_out)
    }
  }
}

result_df <- dplyr::bind_rows(result_list)
result_df

tictoc::toc()
```

```{r}
result_summary <- result_df %>%
    dplyr::group_by(task, feature_set, model, trainset, testset) %>%
    dplyr::summarise(
      test_mse = get_mse(target, prediction),
      test_r2 = get_r2(target, prediction),
      test_srho = get_spearman(target, prediction),
      mse_same_cohort = get_mse(target, testset_mean),
      .groups = "drop"
    )
result_summary
```

```{r}
model_selection_df <- result_summary %>%
  dplyr::select(task, feature_set, model, testset, test_srho) %>%
  tidyr::pivot_wider(values_from=test_srho, names_from=testset) %>%
  dplyr::mutate(srho_mean = apply(select(., matches("[0-9]+_dataset")), 1, mean, na.rm=TRUE)) %>%
  dplyr::mutate(srho_min = apply(select(., matches("[0-9]+_dataset")), 1, min, na.rm=TRUE)) %>%
  dplyr::mutate(srho_sd = apply(select(., matches("[0-9]+_dataset")), 1, sd, na.rm=TRUE)) %>%
  dplyr::mutate(srho_mean = round(srho_mean, 3)) %>%
  dplyr::mutate(srho_min = round(srho_min, 3)) %>%
  dplyr::mutate(srho_sd = round(srho_sd, 3)) %>%
  dplyr::mutate(across(matches("[0-9]+_dataset"), ~ round(.x, 3))) %>%
  dplyr::mutate(dim_red = dplyr::case_when(
    str_detect(feature_set, "_pca") ~ "pca",
    TRUE ~ "none"
  )) %>%
  dplyr::mutate(n_features = (stringr::str_count(feature_set, "\\+") + 1)) %>%
  dplyr::select(task, model, srho_mean, srho_sd, srho_min, !feature_set, feature_set)
```

```{r}
readr::write_delim(model_selection_df, file=file.path(output_dir, "model_selection_df.tsv"), delim="\t")
```




